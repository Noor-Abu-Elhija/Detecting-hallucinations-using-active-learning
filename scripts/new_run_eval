# scripts/new_run_eval_metrics.py

import os
import sys
import json
import random
import numpy as np
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.nli import NLI
from utils.arg_parser import get_args
from src.corpus_index import CorpusIndex
from src.ann_verifier import ANNVerifier
from scripts.build_squad_index import load_squad_qa
from src.semantic_entropy import compute_semantic_entropy
from src.embedding_variance import compute_kmeans_variance
from src.embedding_variance import compute_embedding_variance
from src.generate_with_regular_entropy import compute_regular_entropy
from src.embedding_variance import compute_embedding_variance_weighted
from scripts.generate_answers import load_falcon_model, generate_with_probs, format_prompt

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

def run_generation(question: str, model, tokenizer, num_return_sequences: int, temperature: float, max_new_tokens: int):
    prompt = format_prompt(question)
    completions, sequence_probs = generate_with_probs(prompt, model, tokenizer, num_return_sequences, temperature,
                                                      max_new_tokens)
    return completions, sequence_probs


def evaluate_metrics(
        question: str,
        completions: List[str],
        sequence_probs: List[float],
        embedder: SentenceTransformer,
        metric: str,
        corpus_index: Optional[CorpusIndex] = None,
        ann_threshold: float = 0.9,
        k: int = 5,
        original_question_text: str = "",
        correct_answer: str = "N/A",
        # NEW:
        nli: Optional[NLI] = None,
        ann_nli_mode: str = "lenient",  # "lenient" (v1) or "strict" (v2)
        strict_p_ent: float = 0.80,
        strict_p_con: float = 0.10,
        strict_margin: float = 0.55,
        strict_ann_threshold: float = 0.82
    ) -> Dict[str, Any]:

    results = {
        "question": question,
        "completions": completions,
        "sequence_probs": sequence_probs,
        "correct_answer": correct_answer
    }

    if metric == "semantic_entropy" or metric == "all":
        semantic_ids, H = compute_semantic_entropy(completions, sequence_probs)
        results["semantic_entropy"] = H
        results["semantic_ids"] = semantic_ids

    if metric == "entropy" or metric == "all":
        H = compute_regular_entropy(sequence_probs)
        results["entropy"] = H

    if metric == "variance" or metric == "all":
        centroid, per_var, overall = compute_embedding_variance(embedder.encode(completions))
        results["variance"] = overall

    if metric == "weighted variance" or metric == "all":
        centroid, per_var, overall = compute_embedding_variance_weighted(
            embedder.encode(completions), np.array(sequence_probs)
        )
        results["weighted_variance"] = overall

    if metric == "kmeans variance" or metric == "all":
        labels, cents, per_var, overall, cluster_vars = compute_kmeans_variance(
            embedder.encode(completions), k=k
        )
        results["kmeans_variance"] = overall
        results["cluster_variances"] = cluster_vars

    if metric == "ann" or metric == 'all':
        if corpus_index is None:
            raise ValueError("CorpusIndex is required for ANN metric")
        if nli is None:
            nli = NLI()  # prefer passing from main()

        comp_embs = embedder.encode(completions, convert_to_numpy=True).astype("float32")
        # build a tiny ANN helper on-the-fly from the indexâ€™s embeddings
        # (If your CorpusIndex already exposes search with sims/idxs, you can keep using that.)
        # Here we assume corpus_index.search(embedding, k) -> (sims np.ndarray, idxs np.ndarray)

        ann_details = []
        supported_flags = []

        for i, ce in enumerate(comp_embs):
            sims, idxs = corpus_index.search(ce, k=k)  # expects top-k cosine sims & ids
            sims = sims.tolist()
            idxs = idxs.tolist()
            nearest_texts = [corpus_index.texts[int(j)] for j in idxs] if idxs else []

            hypothesis = f"The answer to the question '{original_question_text}' is '{completions[i]}'."

            # Batch NLI over the k premises
            nli_rows = nli.predict_batch(nearest_texts, hypothesis) if nearest_texts else []

            # Decide support depending on mode
            max_sim = float(sims[0]) if sims else 0.0
            if ann_nli_mode.lower() == "lenient":
                # v1 (your old approach): ANN >= ann_threshold, and ANY (entailment OR neutral)
                nli_any_support = any(r["label"] in ("entailment", "neutral") for r in nli_rows)
                supported = (max_sim >= ann_threshold) and nli_any_support
                # pick the best row for reporting (prefer entailment, then neutral, highest margin)
                best_row = None
                for pref in ("entailment", "neutral"):
                    cand = [r for r in nli_rows if r["label"] == pref]
                    if cand:
                        best_row = max(cand, key=lambda r: (r["margin"], r["conf"]))
                        break
                if best_row is None and nli_rows:
                    best_row = max(nli_rows, key=lambda r: (r["margin"], r["conf"]))
                best_premise = nearest_texts[nli_rows.index(best_row)] if best_row else (nearest_texts[0] if nearest_texts else "")

            else:
                # v2 strict: ANN >= strict_ann_threshold AND strong entailment signal
                # find the passage with best (margin, cosine)
                best_j = -1
                best_key = (-1.0, -1.0)  # (margin, cosine)
                for j, r in enumerate(nli_rows):
                    key = (r["margin"], sims[j])
                    if key > best_key:
                        best_key = key
                        best_j = j
                if best_j == -1:
                    best_row = None
                    best_premise = ""
                    supported = False
                else:
                    best_row = nli_rows[best_j]
                    best_premise = nearest_texts[best_j]
                    supported = (
                        (max_sim >= strict_ann_threshold) and
                        (best_row["p_ent"] >= strict_p_ent) and
                        (best_row["p_con"] <= strict_p_con) and
                        (best_row["margin"] >= strict_margin)
                    )

            supported_flags.append(bool(supported))
            ann_details.append({
                "completion": completions[i],
                "is_supported": bool(supported),
                "max_similarity": max_sim,
                "nearest_sentence": best_premise,
                "nli_label": (best_row["label"] if (ann_nli_mode=="lenient" and 'best_row' in locals() and best_row)
                              else (best_row["label"] if ('best_row' in locals() and best_row) else "n/a")),
                "p_ent": (best_row["p_ent"] if ('best_row' in locals() and best_row) else 0.0),
                "p_con": (best_row["p_con"] if ('best_row' in locals() and best_row) else 0.0),
                "margin": (best_row["margin"] if ('best_row' in locals() and best_row) else 0.0),
                "topk_similarities": sims[:k],
            })

        results["ann_details"] = ann_details
        results["supported_ratio"] = float(sum(supported_flags)) / len(supported_flags) if supported_flags else 0.0

    return results


def load_questions(path: str) -> List[Dict[str, Any]]:
    if not path:
        raise ValueError("A dataset file path must be provided via the --dataset argument.")
    if not os.path.exists(path):
        raise FileNotFoundError(f"Error: The dataset file was not found at the path: {path}")
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]


def main():
    args = get_args()
    all_qa_pairs = load_squad_qa("train")
    qa_pairs = random.sample(all_qa_pairs, args.num_of_question)
    questions = [item["question"] for item in qa_pairs]

    print("Loading models (this may take a moment)...")
    falcon_tokenizer, falcon_model = load_falcon_model()

    embedder = SentenceTransformer(args.embed_model, device='cpu')

    corpus_index = None
    if args.metric in {'ann', 'all'}:
        if not args.index_dir:
            raise ValueError("You must provide --index_dir when using 'ann' or 'all'.")
        print(f"Loading sentence-chunk index from {args.index_dir}...")
        corpus_index = CorpusIndex.load(args.index_dir)

    # NEW: load NLI once
    nli = NLI()
    # if you must force CPU (e.g., PuTTY box without CUDA):
    import torch
    if nli.device != "cpu" and not torch.cuda.is_available():
        nli.model.to('cpu')
        nli.device = 'cpu'

    # Toggle which profile to use
    ANN_NLI_MODE = "lenient"   # "lenient" (your v1) or "strict" (v2)

    all_results = []
    for i, question_text in enumerate(questions):
        print(f"\nProcessing question {i + 1}/{args.num_of_question}: '{question_text}'")

        completions, sequence_probs = run_generation(
            question=question_text,
            model=falcon_model,
            tokenizer=falcon_tokenizer,
            num_return_sequences=args.num_generations,
            temperature=args.temperature,
            max_new_tokens=args.max_new_tokens
        )

        results = evaluate_metrics(
            question=question_text,
            completions=completions,
            sequence_probs=sequence_probs,
            embedder=embedder,
            metric=args.metric,
            corpus_index=corpus_index,
            ann_threshold=args.ann_threshold,   # used by lenient
            k=args.k,
            original_question_text=question_text,
            correct_answer=qa_pairs[i]["answers"],
            nli=nli,
            ann_nli_mode=ANN_NLI_MODE,
            # Strict profile thresholds (only used if ANN_NLI_MODE == "strict"):
            strict_p_ent=0.80,
            strict_p_con=0.10,
            strict_margin=0.55,
            strict_ann_threshold=0.82
        )
        all_results.append(results)

    if args.save_json:
        output_dir = os.path.dirname(args.save_json)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        with open(args.save_json, "w", encoding="utf-8") as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        print(f"\nSaved all {len(all_results)} results to {args.save_json}")


if __name__ == "__main__":
    main()
