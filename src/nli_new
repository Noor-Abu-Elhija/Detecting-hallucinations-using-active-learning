# src/nli_new.py
from __future__ import annotations
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class NLI:
    """
    Natural Language Inference (entailment/neutral/contradiction).
    Default model: 'facebook/bart-large-mnli'.
    """
    def __init__(self, model_name: str = "facebook/bart-large-mnli"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.model.eval()
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(device)
        self.device = device
        # Modelâ€™s logits order:
        self.labels = ["contradiction", "neutral", "entailment"]

    def predict(self, premise: str, hypothesis: str):
        """
        returns: (label:str, conf:float, scores:dict)
        scores has keys: 'contradiction','neutral','entailment'
        """
        inputs = self.tokenizer(premise, hypothesis, return_tensors="pt",
                                truncation=True, padding=True, max_length=512).to(self.device)
        with torch.no_grad():
            logits = self.model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]
        idx = int(probs.argmax())
        label = self.labels[idx]
        scores = dict(zip(self.labels, probs.tolist()))
        return label, float(probs[idx]), scores

    def predict_batch(self, premises: list[str], hypothesis: str):
        """
        Batched NLI: same hypothesis vs. multiple premises.
        returns: list[dict] with keys: label, p_ent, p_con, p_neu, margin, conf
        where margin = p_entailment - p_contradiction
        """
        if not premises:
            return []
        enc = self.tokenizer(
            premises, [hypothesis] * len(premises),
            return_tensors="pt", truncation=True, padding=True, max_length=512
        ).to(self.device)
        with torch.no_grad():
            logits = self.model(**enc).logits
        probs = torch.softmax(logits, dim=-1).cpu().numpy()

        out = []
        for row in probs:
            p_con, p_neu, p_ent = map(float, row.tolist())
            margin = p_ent - p_con
            # predicted label + its prob (conf)
            idx = int(row.argmax())
            label = self.labels[idx]
            conf = float(row[idx])
            out.append({
                "label": label,
                "p_ent": p_ent,
                "p_con": p_con,
                "p_neu": p_neu,
                "margin": margin,
                "conf": conf
            })
        return out
